{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors\n",
    "\n",
    "PyTorch Tensors are just multi-dimensional arrays. You can go back and forth between these and numpy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16208704, 0.98401256],\n",
       "       [0.87045688, 0.3129406 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.rand(2,2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1621, 0.9840],\n",
       "        [0.8705, 0.3129]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.Tensor(A)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put a tensor on gpu, use cuda(). Note that you must have an NVIDIA GPU in your computer to be able to do this successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1621, 0.9840],\n",
      "        [0.8705, 0.3129]], device='cuda:0')\n",
      "tensor([[0.1621, 0.9840],\n",
      "        [0.8705, 0.3129]])\n"
     ]
    }
   ],
   "source": [
    "Bcuda = B.cuda()\n",
    "print(Bcuda)\n",
    "Bcpu = B.cpu()\n",
    "print(Bcpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1621, 0.9840],\n",
       "        [0.8705, 0.3129]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.to(device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move a tensor back to CPU, you can use device='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Functions¶\n",
    "\n",
    "PyTorch provides access to a variety of BLAS and LAPACK-type routines - see [documentation here](https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations). These do not follow the BLAS/LAPACK naming conventions\n",
    "\n",
    "[torch.addmv](https://pytorch.org/docs/stable/generated/torch.addmv.html#torch-addmv) is roughly equivalent to axpy, and performs $ Ax + y $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "n = 100\n",
    "device = torch.device('cuda') # 'cuda' or 'cpu'\n",
    "\n",
    "Anp = np.random.randn(m,n)\n",
    "xnp = np.random.randn(n)\n",
    "ynp = np.random.randn(m)\n",
    "\n",
    "A = torch.Tensor(Anp).to(device=device)\n",
    "x = torch.Tensor(xnp).to(device=device)\n",
    "y = torch.Tensor(ynp).to(device=device)\n",
    "\n",
    "z = torch.addmv(y, A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the timing difference between CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy\n",
      "CPU times: user 8.23 ms, sys: 9.47 ms, total: 17.7 ms\n",
      "Wall time: 2.96 ms\n",
      "\n",
      "device = cpu\n",
      "CPU times: user 23.3 ms, sys: 19.8 ms, total: 43.2 ms\n",
      "Wall time: 6.13 ms\n",
      "\n",
      "device = cuda\n",
      "CPU times: user 303 µs, sys: 107 µs, total: 410 µs\n",
      "Wall time: 88.2 µs\n"
     ]
    }
   ],
   "source": [
    "m = 1000\n",
    "n = 1000\n",
    "\n",
    "Anp = np.random.randn(m,n)\n",
    "xnp = np.random.randn(n)\n",
    "ynp = np.random.randn(m)\n",
    "print(\"numpy\")\n",
    "%time z = ynp + Anp @ xnp\n",
    "\n",
    "\n",
    "for device in ('cpu', 'cuda'):\n",
    "    print(f\"\\ndevice = {device}\")\n",
    "    A = torch.Tensor(Anp).to(device=device)\n",
    "    x = torch.Tensor(xnp).to(device=device)\n",
    "    y = torch.Tensor(ynp).to(device=device)\n",
    "    z = torch.addmv(y, A, x)\n",
    "\n",
    "    %time z = torch.addmv(y, A, x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[torch.mv](https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv) performs matrix-vector products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy\n",
      "CPU times: user 4.91 ms, sys: 1.77 ms, total: 6.68 ms\n",
      "Wall time: 1.1 ms\n",
      "\n",
      "device = cpu\n",
      "CPU times: user 22.4 ms, sys: 3.39 ms, total: 25.7 ms\n",
      "Wall time: 4.53 ms\n",
      "\n",
      "device = cuda\n",
      "CPU times: user 521 µs, sys: 199 µs, total: 720 µs\n",
      "Wall time: 110 µs\n"
     ]
    }
   ],
   "source": [
    "Anp = np.random.randn(m,n)\n",
    "xnp = np.random.randn(n)\n",
    "\n",
    "print(\"numpy\")\n",
    "%time z = Anp @ xnp\n",
    "\n",
    "\n",
    "for device in ['cpu', 'cuda']:\n",
    "    print(f\"\\ndevice = {device}\")\n",
    "    A = torch.Tensor(Anp).to(device=device)\n",
    "    x = torch.Tensor(xnp).to(device=device)\n",
    "    y = torch.mv(A, x)\n",
    "\n",
    "    %time y = torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[torch.mm](https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm) performs matrix-matrix multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy\n",
      "CPU times: user 125 ms, sys: 31.4 ms, total: 156 ms\n",
      "Wall time: 23.1 ms\n",
      "\n",
      "device = cpu\n",
      "CPU times: user 24.2 ms, sys: 0 ns, total: 24.2 ms\n",
      "Wall time: 6.54 ms\n",
      "\n",
      "device = cuda\n",
      "CPU times: user 221 µs, sys: 0 ns, total: 221 µs\n",
      "Wall time: 224 µs\n"
     ]
    }
   ],
   "source": [
    "m = 1000\n",
    "n = 1000\n",
    "\n",
    "Anp = np.random.randn(m,n)\n",
    "Bnp = np.random.randn(n, n)\n",
    "\n",
    "print(\"numpy\")\n",
    "%time C = Anp @ Bnp\n",
    "\n",
    "for device in ['cpu', 'cuda']:\n",
    "    print(f\"\\ndevice = {device}\")\n",
    "    A = torch.Tensor(Anp).to(device=device)\n",
    "    B = torch.Tensor(Bnp).to(device=device)\n",
    "    C = torch.mm(A, B) # run once to warm up\n",
    "\n",
    "    %time C = torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch operations\n",
    "\n",
    "Where PyTorch (and GPUs in general) really shine are in batch operations. We get extra efficiency if we do a bunch of multiplications with matrices of the same size.\n",
    "\n",
    "For matrix-matrix multiplcation, the function is [torch.bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm)\n",
    "\n",
    "Because tensors are row-major, we want the batch index to be the first index. In the below code, the batch multiplication is equivalent to\n",
    "```\n",
    "for i in range(k):\n",
    "    C[i] = A[i] @ B[i]\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy\n",
      "CPU times: user 897 ms, sys: 791 ms, total: 1.69 s\n",
      "Wall time: 250 ms\n",
      "\n",
      "device = cpu\n",
      "CPU times: user 194 ms, sys: 11.1 ms, total: 205 ms\n",
      "Wall time: 56.3 ms\n",
      "\n",
      "device = cuda\n",
      "CPU times: user 263 µs, sys: 121 µs, total: 384 µs\n",
      "Wall time: 393 µs\n"
     ]
    }
   ],
   "source": [
    "n = 512 # matrix size\n",
    "k = 32 # batch size\n",
    "\n",
    "Anp = np.random.randn(k, n, n)\n",
    "Bnp = np.random.randn(k, n, n)\n",
    "# see numpy matmul documentation for how this performs batch multiplication\n",
    "print(\"numpy\")\n",
    "%time C = np.matmul(Anp, Bnp)\n",
    "\n",
    "for device in ['cpu', 'cuda']:\n",
    "    print(f\"\\ndevice = {device}\")\n",
    "    A = torch.randn(k, n, n).to(device=device)\n",
    "    B = torch.randn(k, n, n).to(device=device)\n",
    "    C = torch.bmm(A, B) # run once to warm up\n",
    "\n",
    "    %time C = torch.bmm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Linear Algebra¶\n",
    "\n",
    "PyTorch also supports sparse tensors in [torch.sparse](https://pytorch.org/docs/stable/sparse.html). Tensors are stored in [COOrdinate format](https://caam37830.github.io/book/02_linear_algebra/sparse.html#coordinate-format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 3.],\n",
       "        [4., 0., 5.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.LongTensor([[0, 1, 1],\n",
    "                      [2, 0, 2]])\n",
    "v = torch.FloatTensor([3, 4, 5])\n",
    "torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indices are stored in a 2 x nnz tensor of Long (a datatype that stores integers). Values are stored as floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
